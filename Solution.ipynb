{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the problem\n",
    "Since we assume, that we won't know the weather beforehand, the problem could be modeled as the Hidden Markov Model in the following way: <br>\n",
    "we have a time series of the weather attributes, $X_t$. These influence the number of calls: $X_t \\to Y_t$.<br>\n",
    "$\\quad Y_t \\quad Y_{t+1}$ <br>\n",
    "$\\quad \\hat{|}\\qquad \\hat{|}$ <br>\n",
    "$\\to X_t \\to X_{t+1} \\to ...$ <br>\n",
    "$Y_t$ is also influenced by the some other factors $\\tilde X_{t}$, related to human activity, e.g. weekend or season, which are known beforehand for each timestamp. <br>\n",
    "In the end, our problem consist of 2 problems:<br>\n",
    "a) predict $X_{t+1}$ based on $X_t, X_{t-1},....$<br>\n",
    "b) find $f$ s.t. $f(X_{t+1},\\tilde X_{t+1})=Y_{t+1}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each attribute of the weather is a separate time series, in order to avoid extra work, we will start with b) to understand which time series $X_t$ we don't need to analyze, as they don't contribute much to the model in a)\n",
    "# Our plan\n",
    "\n",
    "### 1) derive/predict the number of calls given the weather, time range\n",
    "\n",
    "For this exactly we will use the **Gradient Boosting** method. Using it we will see which weather attributes contribute most to the model and work with them in the step 2: <br>\n",
    "\n",
    "###  2) Every weather attribute (selected from 1) is a separate time series, for which we will need the prediction. \n",
    "(But we will come to this later)\n",
    " Here we will use **ARMA or ARIMA** models, depending on the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Exploring & preprocessing raw data\n",
    "Please find the notebook **'Step1_process_raw_data.ipynb'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature engineering\n",
    "### Step 2.1 Calls\n",
    "Please find the notebook  **'Step2_calls_data.ipynb'**\n",
    "### Step 2.2 Weather\n",
    "Please find the notebook  **'Step2_weather_data.ipynb'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Model: Predict #calls given weather and date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting number of calls based on the weather and other attributes\n",
    "\n",
    "What data we need: <br>\n",
    "Indep. variables <br>\n",
    "-- weather attributes<br>\n",
    "-- extra categorical attributes, derived from the timestamp, that could reflect human activity (see the analysis of time series of number of calls):<br>\n",
    "* hour of the day \n",
    "* day of the week  \n",
    "* month of the year \n",
    "Note that we include it not because of the weather, but because of the human activity at different time of year, e.g. in summer people do more campfires than in winter<br>\n",
    "* season\n",
    "\n",
    "\n",
    "Depend.var.<br>\n",
    "-- number of calls at time t<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import pyexasol\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import statsmodels as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummies(df,attribute):\n",
    "    # to have categorical values, we need to have the datatype string\n",
    "    #making sure the datatype is string\n",
    "    conv_to_str = lambda x: str(x)\n",
    "    df[attribute]=df[attribute].apply(conv_to_str)\n",
    "    dummies = pd.get_dummies(df[[attribute]], drop_first=True) #  #dummy var. = #categorical values - 1\n",
    "    df = df.drop(attribute,axis=1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = pd.read_csv('files/weather_to_numb_calls.csv',sep=';',decimal=',')\n",
    "ds_raw= ds_raw.drop_duplicates(subset=['DT_ISO'])\n",
    "ds_raw = ds_raw.reset_index(drop=True)\n",
    "ds_raw['DT_ISO'] = pd.to_datetime(ds_raw.DT_ISO)\n",
    "# as discussed before we exclude it, as it's highly correlated\n",
    "columns_to_drop = ['FEELS_LIKE','TEMP_MIN','TEMP_MAX']\n",
    "ds_raw = ds_raw.drop(columns_to_drop,axis = 1) \n",
    "ds_raw = ds_raw.rename(columns={\"WEATHER_DESCRIPTION\": \"wd\"}) # shorten the name beforehand\n",
    "categorical_attr=['wd','H','D','M','S']\n",
    "for attr in categorical_attr:\n",
    "    ds_raw=add_dummies(ds_raw,attr)\n",
    "# some column names have extra spaces in their name\n",
    "for col in ds_raw.columns:\n",
    "    ds_raw = ds_raw.rename(columns={col: col.strip()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & evaluate the model\n",
    "#### Rmk\n",
    "To be fair, we won't use the data, that will be our test set for the problem of time series prediction. Thus we should exclude the last year: from 2019-11-01 till 2020-11-01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_raw[(ds_raw['DT_ISO']<'2019-11-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.drop(['DT_ISO','NUMB_CALLS'], axis = 1) #'BID',,'BIDDING_MODE'\n",
    "y = ds['NUMB_CALLS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply **gradient boosting for regression** and we will use the scikit-learn library. We will use the **cross validation** to evaluate our model, where the evaluation metric we chose are: **mean squared error and mean absolute error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(X, y,use_params=False):\n",
    "    reg = GradientBoostingRegressor()\n",
    "    reg.fit(X, y)\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    n_scores_mse = cross_val_score(reg, X, y, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    n_scores_mae = cross_val_score(reg, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('MSE: mean %.3f, st.dev %.3f' % (np.mean(n_scores_mse), np.std(n_scores_mse)))\n",
    "    print('MAE: mean %.3f, st.dev %.3f' % (np.mean(n_scores_mae), np.std(n_scores_mae)))\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first let us see the result with the parameters chosen by default in scikit learn for Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = fit_evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the histograms of calls as a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, density=True, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls are normally distributed with mean around 12 and range roughly between 0 and 30. <br>\n",
    "Having the absolute error mean 3 that our model is normally off by 3 calls per hour in average. A pretty small standart deviation means this error doesn't deviate much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find out the **most important features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "feature_importance = model_gb.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(X_train.columns)[sorted_idx])\n",
    "plt.title('Feature Importance (MDI)')\n",
    "\n",
    "result = permutation_importance(model_gb, X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=np.array(X_train.columns)[sorted_idx])\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impurity-based and permutation methods show that identifying the **hour of the day and TEMP** are the most significant attributes. As expected we also see **D_Saturday,D_Sunday and S_summer** (which makes sense, as people go out then more). Both graphs show that the **humidity** are also important, followed by **wind_speed, wind_deg and clouds_all**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if leaving only the most important features would affect significantly the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [col for col in X.columns if col.startswith('H_') ]\n",
    "other_features=['D_sunday','D_saturday','TEMP','S_summer']\n",
    "selected_features.extend(other_features)\n",
    "model_gb_simpler = fit_evaluate(X[selected_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously with all the features we had an error: <br>\n",
    "MSE: mean -15.100, st.dev 0.346 <br>\n",
    "MAE: mean -3.025, st.dev 0.036 <br>\n",
    "As we can see, our model's error barely changed after excluding many parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means, that now among all the weather parameters, *we will need to predict only* **temperature**. If we found out, that more weather attributes contributed significantly, we would analyze each attributes as a separate time series and have a separate prediction model for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also apply the grid search to find optimal hyperparameters such as learning rate, maximum number of trees, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Predict $X_{t+1}$ (temperature) from $X_t$\n",
    "## Predicting temperature \n",
    "\n",
    "We will identify the properties of the temperature time series, decide which lagged information could be used and other additional useful attributes. Once again, we will use the data till November 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_attr(df,n_lags=50):\n",
    "    #plt.rcParams['figure.max_open_warning'] = 0\n",
    "    weather_time_series = [col for col in df.columns if col not in ('NUMB_CALLS','DT_ISO') ]\n",
    "    for col in weather_time_series:\n",
    "        plot_acf(df[col],lags=n_lags)\n",
    "        plot_pacf(df[col], lags=n_lags)\n",
    "        plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The **Dickey-Fuller Test** whether this timeseries is **stationary**, \n",
    "# i.e. there is no trend or seasonal behaviour (though the series could be cyclic).\n",
    "def check_stationarity(df):\n",
    "    print(\" > Is the data stationary ?\")\n",
    "    dftest = adfuller(df, autolag='AIC')\n",
    "    print(\"Test statistic = {:.3f}\".format(dftest[0]))\n",
    "    print(\"P-value = {:.3f}\".format(dftest[1]))\n",
    "    print(\"Critical values :\")\n",
    "    for k, v in dftest[4].items():\n",
    "        print(\"\\t{}: {} - The data is {} stationary with {}% confidence\".format(k, v, \"not\" if v<dftest[0] else \"\", 100-int(k[:-1])))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds contains data till November 2019\n",
    "temp = ds[['DT_ISO','TEMP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity(temp['TEMP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the test showed that our data is stationary, this test assumes that there could be only linear trend in the data and thus could be mistaken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df,one_col='', omit=[],normalize=False,span=0,calls=False):\n",
    "    if span>0:\n",
    "        df = df.set_index(\"DT_ISO\", inplace=False) # inplace=F: make a copy, don't modify directly\n",
    "        df = df.rolling(span).mean()\n",
    "        x_axis = df.index\n",
    "    else:\n",
    "        x_axis = df['DT_ISO'][:]\n",
    "    omit.extend(['DT_ISO'])\n",
    "    columns_time_series=[one_col]\n",
    "    if len(one_col)==0:\n",
    "        columns_time_series = [col for col in df.columns if col not in omit ]\n",
    "    if calls:\n",
    "        columns_time_series.extend(['NUMB_CALLS'])\n",
    "    plt.figure(\"figure\",figsize=(40,15))\n",
    "    for col in columns_time_series:\n",
    "        if normalize:\n",
    "            y_axis = (10*df[col]/df[col].max())[:]\n",
    "            # normalize the output, so all the attributes are between 0 and 10 \n",
    "        else:\n",
    "            y_axis = df[col][:]\n",
    "        plt.plot(x_axis,y_axis)\n",
    "    plt.legend(columns_time_series)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(temp,one_col='TEMP', omit=[],normalize=False,span=0,calls=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each year separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_2016=temp[(temp['DT_ISO']<'2017-01-01')&(temp['DT_ISO']>='2016-01-01')]\n",
    "year_2017=temp[(temp['DT_ISO']>='2017-01-01')&(temp['DT_ISO']<'2018-01-01')]\n",
    "year_2018=temp[(temp['DT_ISO']>='2018-01-01')&(temp['DT_ISO']<'2019-01-01')]\n",
    "plot_time_series(year_2016,one_col='TEMP',normalize=False,span=15)\n",
    "plot_time_series(year_2017,one_col='TEMP',normalize=False,span=15)\n",
    "plot_time_series(year_2018,one_col='TEMP',normalize=False,span=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposed = seasonal_decompose(temp.TEMP, model='additive',extrapolate_trend='freq',freq=365*24)\n",
    "plt.rcParams.update({'figure.figsize':(10,10)})\n",
    "decomposed.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the time series has **yearly seasonality** and possibly a declining trend, thus is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see the autocorrelation for one week\n",
    "autocorrelation_attr(temp,n_lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the temperature at hour t very strongly depends on the **previous hour/hours** and the tempreture **24 hours before (daily seasonality)**. This is also backed by the partial autocorrelation function. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have the seasonality, unless we remove it, we cannot use ARIMA model for this problem. <br>\n",
    "However, in that case best model to represent this time series is **seasonal ARIMA** or SARIMA. <br>\n",
    "\n",
    "Except for the ARIMA's parameters:\n",
    "* p: Trend autoregression order.\n",
    "* d: Trend difference order.\n",
    "* q: Trend moving average order \n",
    "\n",
    "we also need to define the seasonal elements:\n",
    "\n",
    "* P: Seasonal autoregressive order.\n",
    "* D: Seasonal difference order.\n",
    "* Q: Seasonal moving average order.\n",
    "* m: The number of time steps for a single seasonal period. \n",
    "\n",
    "To sum up, we have a model, where $X_t$ depends on $X_{t-1}, X_{t-24}$ and $X_{t-365-6}$ (Year has 365+6 hours) <br>\n",
    "In our case we have daily and yearly seasonality, thus **m = 24** <br>\n",
    "We added 6, as each year has 365 days and 6 hours and the temperature is not \"aware\" about the leap year.<br>\n",
    "In that case P = how many days before we will take into consideration. E.g. P=2 => (t-1 days),(t-2 days) <br>\n",
    "We will find out what parameters fit best for our model using the **grid search**\n",
    "\n",
    "\n",
    "**Rmk**: Seasonal ARIMA is designed for shorter periods such as 12 for monthly data or 24 for hourly data. In order to capture the yearly seasonality in hourly data, m has to be tremendously large 24 x 365+6, which is very a computationally heavy (requires too much RAM). This way we expect that our model might not capture the yearly seasonality, but will predict better for short term timerange.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based on one year of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our model on **one year** of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_year=temp[(temp['DT_ISO']>='2018-11-01')&(temp['DT_ISO']<'2019-11-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case pmdarima is not installed\n",
    "try:\n",
    "    import pmdarima as pm\n",
    "except:\n",
    "    !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_SARMAX(data):\n",
    "    p = range(0, 3)\n",
    "    d = range(1,2)\n",
    "    q = range(0, 3)\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], 24) for x in list(itertools.product(p, d, q))]\n",
    "    print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "    print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "    print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "    print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "    print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            mod = sm.tsa.statespace.sarimax.SARIMAX(data['TEMP'],\n",
    "                                                order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "###  DUE to the limitation of RAM, this was run on Google Colab (see the output in the next cell)\n",
    "### \n",
    "results = grid_search_SARMAX(training_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model **SARIMA(1,1,1) (1,1,1,24)** has the lowest AIC thus that is our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SARIMAX model using optimal parameters\n",
    "mod = sm.tsa.statespace.sarimax.SARIMAX(training_year['TEMP'],\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 1, 1, 24),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the residuals seem to be normally distributed, the correlogram shows that some residuals are correlated with some lags. As we said before Seasonal ARIMA is designed to learn short periods of seasonality. We could tackle it by applying the Fourier series approach.  <br> \n",
    "(However we will stop here due to time and memory limitations). <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SARIMAX model with the correct time index\n",
    "training_year_index = training_year.set_index(\"DT_ISO\", inplace=False)\n",
    "training_year_index.index = pd.DatetimeIndex(training_year_index.index).to_period('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.statespace.sarimax.SARIMAX(training_year_index,\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 1, 1, 24),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting\n",
    "One of the form of backtesting is **walk forward validation**. We will make a prediction for one hour ehead and then use this prediction for predicting further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "# in start and end we need datetime.datetime \n",
    "prediction = results.get_prediction(start= dt.datetime(2019, 11, 1,0,0), \n",
    "                            end= dt.datetime(2020, 11, 1,0,0),\n",
    "                            dynamic=False) \n",
    "# The dynamic=False argument ensures that we produce one-step ahead forecasts, \n",
    "# meaning that forecasts at each point are generated using the full history up to that point.\n",
    "prediction_ci = prediction.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get the training & test data\n",
    "testing_year = ds_raw[(ds_raw['DT_ISO']>='2019-11-01')][['DT_ISO','TEMP']]\n",
    "testing_year_index = testing_year.set_index(\"DT_ISO\", inplace=False)\n",
    "testing_year_index.index = pd.DatetimeIndex(testing_year_index.index).to_period('H')\n",
    "# combine the test year (2020) and the training years (2018,2019) \n",
    "all_series = pd.concat([training_year_index, testing_year_index], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case some duplicate indices appeared\n",
    "all_series = all_series[~all_series.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10)})\n",
    "ax = all_series['2018-11-01':].plot(label='observed')\n",
    "prediction.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.fill_between(prediction_ci.index,\n",
    "                    prediction_ci.iloc[:, 0],\n",
    "                    prediction_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval is growing very rapidly and the prediction becomes very uncertain even at the early stage.Let us take a closer look at the prediction for one month: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10)})\n",
    "ax = all_series['2018-11-01':].plot(label='observed')\n",
    "prediction.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, that model cannot capture the long-term seasonality, as it is based on the short-term ones. As we stated above, we did not train SARIMA with very high the parameter m=365x24 that would capture yearly seasonality due to memory limitations. <br> <br>\n",
    "It is a known fact, that ARIMA models converge either to straight line, non-zero constant or to mean of the data, depending on the parameters of the model. That is what we see in our case: at first the model makes more reasonable predictions and then just follows a straight line. <br>\n",
    "\n",
    "Let us look closer at the prediction in November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10)})\n",
    "prediction2 = results.get_prediction(start= dt.datetime(2019, 11, 1,0,0), \n",
    "                            end= dt.datetime(2019, 12, 1,0,0),\n",
    "                            dynamic=False)\n",
    "ax = all_series['2019-10-01':'2019-12-01'].plot(label='observed')\n",
    "prediction2.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction mimics the behaviour from around 1 week ago, where we observe the declining trend in real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean square error\n",
    "y_forecasted = prediction2.predicted_mean[:-1] # the last element is redundant\n",
    "y_truth = all_series['2019-11-01':'2019-11-30']\n",
    "mse = np.sqrt(MSE(y_truth, y_forecasted).mean())\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction for a week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10)})\n",
    "prediction3 = results.get_prediction(start= dt.datetime(2019, 11, 1,0,0), \n",
    "                            end= dt.datetime(2019, 11, 8,0,0),\n",
    "                            dynamic=False)\n",
    "ax = all_series['2019-10-20':'2019-11-08'].plot(label='observed')\n",
    "prediction_ci_3 = prediction3.conf_int()\n",
    "prediction3.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.fill_between(prediction_ci_3.index,\n",
    "                    prediction_ci_3.iloc[:, 0],\n",
    "                    prediction_ci_3.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean square error\n",
    "y_forecasted = prediction3.predicted_mean[:-1] # the last element is redundant\n",
    "y_truth = all_series['2019-11-01':'2019-11-07']\n",
    "mse = np.sqrt(MSE(y_truth, y_forecasted).mean())\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based on one month of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_month = training_year[training_year['DT_ISO']>'2019-10-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_SARMAX(training_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the model  **ARIMA(1, 1, 2)x(1, 1, 2, 24)** with AIC score 970."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SARIMAX model with the correct time index\n",
    "model = sm.tsa.statespace.sarimax.SARIMAX(training_month['TEMP'],\n",
    "                                order=(1, 1, 2),\n",
    "                                seasonal_order=(1, 1, 2, 24),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results_month = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_month.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this model performance looks better than the previous one: here we see that the residuals are normally distributed with the mean zero and almost not autocorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_month_index = training_year_index['2019-10-01':'2019-10-31']\n",
    "training_month_index = training_month_index[~training_month_index.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SARIMAX model with the correct time index\n",
    "model = sm.tsa.statespace.sarimax.SARIMAX(training_month_index,\n",
    "                                order=(1, 1, 2),\n",
    "                                seasonal_order=(1, 1, 2, 24),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results_month = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in start and end we need datetime.datetime \n",
    "prediction_month = results_month.get_prediction(start= dt.datetime(2019, 11, 1,0,0), \n",
    "                            end= dt.datetime(2019, 12, 1,0,0),\n",
    "                            dynamic=False) \n",
    "# The dynamic=False argument ensures that we produce one-step ahead forecasts, \n",
    "# meaning that forecasts at each point are generated using the full history up to that point.\n",
    "prediction_ci_month = results_month.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_month.predicted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_series = all_series[~all_series.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10)})\n",
    "ax = all_series['2019-10-01':'2019-12-01'].plot(label='observed')\n",
    "prediction_month.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts well only for some next days, but captures a decreasing trend. <br> \n",
    "We can see the the error is smaller in comparison to the model trained on more data. (see above) <br>\n",
    "As an idea, to improve further the result we could test the Fourier series approach or even using Gradient Boosting with variables, denoting the seasonal patterns (in our case yearly and daily patterns of temperature change), but for now we will keep the last SARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean square error\n",
    "y_forecasted = prediction_month.predicted_mean[:-1] # the last element is redundant\n",
    "y_truth = all_series['2019-11-01':'2019-11-30']\n",
    "mse = np.sqrt(MSE(y_truth, y_forecasted).mean())\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict for a week**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'figure.figsize':(20,10)})\n",
    "prediction_week = results_month.get_prediction(start= dt.datetime(2019, 11, 1,0,0), \n",
    "                            end= dt.datetime(2019, 11, 8,0,0),\n",
    "                            dynamic=False)\n",
    "ax = all_series['2019-10-27':'2019-11-07'].plot(label='observed')\n",
    "prediction_week_ci = prediction_week.conf_int()\n",
    "prediction_week.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "ax.fill_between(prediction_week_ci.index,\n",
    "                    prediction_week_ci.iloc[:, 0],\n",
    "                    prediction_week_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Temp')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prediction_week.predicted_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean square error\n",
    "y_forecasted = prediction_week.predicted_mean[:-1] # the last element is redundant\n",
    "y_truth = all_series['2019-11-01':'2019-11-07']\n",
    "mse = np.sqrt(MSE(y_truth, y_forecasted).mean())\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The predictions for a week and a month are at about as precise for both models: the one trained with one month of data and the one trained with one year of data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last step: Evaluate both models working together\n",
    "\n",
    "As we are going to use the **model trained on one month** and the simpler model of Gradient boosting. <br>\n",
    "\n",
    "The Gradient boosting model will take as an input <br>\n",
    "\n",
    "date | temperature | hour | is Sunday | is Saturday | is summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use our predictor for temperature trained on one month\n",
    "# results_month\n",
    "\n",
    "# and the above trained Gradient boosting model \n",
    "# model_gb_simpler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time_input(time_str):\n",
    "    try:\n",
    "        time_from = datetime.strptime(time_from_str, \"%Y-%m-%d %H\")\n",
    "    except:\n",
    "        print(\"Please input the correct format: YYYY-MM-DD HH\")\n",
    "        print(\"E.g. 2019-11-01 01\")\n",
    "        return False\n",
    "    return True      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_temperature(time_from_str, time_till_str): # 'YYYY-MM-DD HH'\n",
    "    if validate_time_input(time_from_str) and validate_time_input(time_till_str):\n",
    "        time_from = datetime.strptime(time_from_str, \"%Y-%m-%d %H\")\n",
    "        time_till = datetime.strptime(time_till_str, \"%Y-%m-%d %H\")\n",
    "    else:\n",
    "        return\n",
    "    start = dt.datetime(time_from.year,time_from.month,time_from.day,time_from.hour,0)\n",
    "    end = dt.datetime(time_till.year,time_till.month,time_till.day,time_till.hour,0)\n",
    "    prediction = results_month.get_prediction(start= start,end= end,dynamic=False)\n",
    "    return prediction.predicted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['H_1', 'H_10', 'H_11', 'H_12', 'H_13', 'H_14', 'H_15', 'H_16', 'H_17', 'H_18', \\\n",
    "                     'H_19', 'H_2', 'H_20', 'H_21', 'H_22', 'H_23', 'H_3', 'H_4', 'H_5', 'H_6', 'H_7', \\\n",
    "                     'H_8', 'H_9', 'D_sunday', 'D_saturday', 'TEMP', 'S_summer']\n",
    "\n",
    "def define_input(time_from_str, time_till_str):\n",
    "    if validate_time_input(time_from_str) and validate_time_input(time_till_str):\n",
    "        time_from = datetime.strptime(time_from_str, \"%Y-%m-%d %H\")\n",
    "        time_till = datetime.strptime(time_till_str, \"%Y-%m-%d %H\")\n",
    "    else:\n",
    "        return\n",
    "    time_range = pd.Series(pd.date_range(start = time_from_str,end=time_till_str, freq='H'))\n",
    "    input_df = pd.DataFrame(0, index=np.arange(len(time_range)), columns=selected_features)\n",
    "    input_df['DT_ISO']=time_range\n",
    "    hours = input_df.DT_ISO.dt.hour\n",
    "    months = input_df.DT_ISO.dt.month\n",
    "    days_of_week=input_df.DT_ISO.dt.dayofweek\n",
    "    i=0\n",
    "    for i in range(len(input_df)):\n",
    "        if hours[i]>0: # hour=0 corresponds to H_1=0...H_23=0\n",
    "            col_h='H_'+str(hours[i])\n",
    "            input_df.loc[i,col_h]=1\n",
    "        if months[i]>=6 and months[i]<=8:\n",
    "            input_df.loc[i,'S_summer']=1\n",
    "        if days_of_week[i]==6:\n",
    "            input_df.loc[i,'D_sunday']=1\n",
    "        elif days_of_week[i]==5:\n",
    "            input_df.loc[i,'D_saturday']=1\n",
    "    predicted_temp = predict_temperature(time_from_str, time_till_str)\n",
    "    if len(predicted_temp)==len(input_df['TEMP']):\n",
    "        for i in range(len(X_validate['TEMP'])):\n",
    "            input_df.loc[i,'TEMP'] = predicted_temp[i]\n",
    "    input_df = input_df.drop(['DT_ISO'],axis=1)\n",
    "    return input_df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_vector(df_real,time_from_str, time_till_str):\n",
    "    if validate_time_input(time_from_str) and validate_time_input(time_till_str):\n",
    "        df_validate = df_real[(df_real['DT_ISO']>=time_from_str) & (df_real['DT_ISO']<=time_till_str)]\n",
    "        return df_validate['NUMB_CALLS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(df_real,time_from_str, time_till_str,plot=False):\n",
    "    if validate_time_input(time_from_str) and validate_time_input(time_till_str):\n",
    "        input_df = define_input(time_from_str,time_till_str)\n",
    "        validation_vector = get_validation_vector(df_real,time_from_str,time_till_str)\n",
    "        call_prediction = model_gb_simpler.predict(input_df)\n",
    "        mse = np.sqrt(MSE(validation_vector, call_prediction).mean())\n",
    "        return call_prediction, mse\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_from_str = '2019-11-01 01'\n",
    "time_till_str ='2020-10-31 23'\n",
    "call_prediction, mse = test_model(ds_raw,time_from_str, time_till_str,plot=True)\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
