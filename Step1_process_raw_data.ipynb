{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Exploring & preprocessing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_weather_raw = pd.read_csv('files/weather_data.csv',sep=',',decimal='.')\n",
    "df_calls_raw = pd.read_csv('files/calls_data.csv',sep=',',decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are given the hourly information about the weather in Seatle, we won't differentiate the calls for different locations in Seatle. Thus in the call dataset we will remove all the columns, **except the datetime and the incident number** (as we assume there could be a few calls at the same time). <br><br>\n",
    "When it comes to the data about weather, we will get rid of redundant columns: **dt, timezone, city_name, lat, lon, sea_level, grnd_level, weather_id, weather_icon and weather_main** (as this information is already included in weather_description).  <br>\n",
    "(A little observation: it's not clear, how there are different timezones in the dataset, while the latitude and longitute do not change.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_weather = ['dt', 'timezone', 'lat', 'lon', 'sea_level', 'grnd_level', 'weather_id', 'weather_icon', \\\n",
    "                   'weather_main', 'city_name']\n",
    "columns_to_drop_calls = ['Address','Type','Latitude','Longitude','Report Location']\n",
    "df_weather_raw = df_weather_raw.drop(columns_to_drop_weather,axis = 1) \n",
    "df_calls_raw = df_calls_raw.drop(columns_to_drop_calls,axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values calls?')\n",
    "display(df_calls_raw.isnull().any())\n",
    "print('Missing values weather?')\n",
    "display(df_weather_raw.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing weather data about the **rain_1h, rain_3h, snow_1h, snow_3h** can be replaced with zeros, which is clear from the values of the weather description. (No rain or snow in the description for the rows with this missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_raw = df_weather_raw.fillna(0)\n",
    "display(df_weather_raw.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's adjust the datestamp of the calls to the standard 'YYYY-MM-DD HH:MM:SS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls_raw['Datetime'] = pd.to_datetime(df_calls_raw.Datetime)\n",
    "df_calls_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for weather data\n",
    "# convert 2002-01-01 00:00:00 +0000 UTC to 2002-01-01 00:00:00\n",
    "import re\n",
    "example= '2002-01-01 00:00:00 +0000 UTC'\n",
    "pos = example.find('+') # find the position of +\n",
    "remove_ending = lambda x: x[:pos-1] # remove everything after +\n",
    "df_weather_raw['dt_iso'] = df_weather_raw['dt_iso'].apply(remove_ending)\n",
    "df_weather_raw['dt_iso'] = df_weather_raw['dt_iso'].apply(pd.to_datetime)\n",
    "df_weather_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the dublicates wrt to timestamp for the weather\n",
    "# since we capture randomly the temperature at one time at one hour, it is fair to choose one value out of two\n",
    "# but we could also take an average\n",
    "df_weather_raw = df_weather_raw.drop_duplicates(subset=['dt_iso'])\n",
    "df_weather_raw =df_weather_raw.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_weather_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_weather_raw['dt_iso'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find out if we are missing any timestamps in the call and weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls_raw['tDiff'] = df_calls_raw.Datetime.diff()\n",
    "df_calls_raw[df_calls_raw.tDiff > pd.Timedelta('1H')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_raw['tDiff'] = df_weather_raw.dt_iso.diff()\n",
    "df_weather_raw[df_weather_raw.tDiff > pd.Timedelta('1H')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, the fire department doesn't receive calls every hour. This means, that when we combine the calls and weather data, we can simply use **left join** and substitute the missing call values with zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the data for the past 5 years: from 2015-11-01 till 2020-11-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls_raw = df_calls_raw[(df_calls_raw['Datetime']>='2015-11-01 00:00:00') & (df_calls_raw['Datetime']<'2020-11-01 00:00:00')]\n",
    "df_weather_raw = df_weather_raw[(df_weather_raw['dt_iso']>='2015-11-01 00:00:00') & (df_weather_raw['dt_iso']<'2020-11-01 00:00:00')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next:\n",
    "Now we will aggregate the information about the calls per hour and join it with the weather information. As suggested, we will be using the database for feature engineering. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time we could execute the queries from Python, however as it cannot be reproduced, the SQL queries used and the output (csv file) of the queries is going to be provided.<br>\n",
    "Just for the demonstration, this cell (not excutable) shows an example how we could work with the database from Python, that "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pyexasol\n",
    "C = pyexasol.connect(dsn='XX.XX.XXX.XXX..XXX:PPPP', user='ira', password='xxxxxxxxxxxx')\n",
    "# aggregate data for calls\n",
    "query_time_series_hour = ' SELECT TRUNC(datetime,\\'HH24\\') date_hour,\\\n",
    "                        count(incident_number) numb_calls\\\n",
    "                        FROM my_schema.calls_data \\\n",
    "                        WHERE TO_DATE(datetime) BETWEEN \\'2015-11-01\\' AND \\'2020-11-01\\'\\\n",
    "                        GROUP BY 1 ORDER BY 1'\n",
    "calls_time_series_hourly = C.export_to_pandas(query_time_series_hour)\n",
    "C.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calls_raw = df_calls_raw.drop('tDiff',axis = 1)\n",
    "df_weather_raw = df_weather_raw.drop('tDiff',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import these files to use them in the Database\n",
    "#df_weather_raw.to_csv('weather_import_db.csv') \n",
    "#df_calls_raw.to_csv('calls_import_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query(file_name):\n",
    "    print('Would you like to see the query? y/n')\n",
    "    ans = input()\n",
    "    if ans=='y':\n",
    "        with open('files/queries/'+file_name, 'r') as file:\n",
    "            query_time_series_hour = file.read()\n",
    "        print('-'*50)\n",
    "        print(query_time_series_hour)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_query('time_series_analysis.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
